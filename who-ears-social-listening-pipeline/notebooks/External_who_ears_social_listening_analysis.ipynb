{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note: Please have a look at the original git repo for documentation and the initial data ingestion pipeline for the source data and references here https://github.com/ShawnKyzer/who-ears-social-listening"
      ],
      "metadata": {
        "id": "nxSe1Qil4Ehn"
      },
      "id": "nxSe1Qil4Ehn"
    },
    {
      "cell_type": "markdown",
      "id": "ac80d806-6f8e-4a60-a9f7-c9ea6f81aa91",
      "metadata": {
        "id": "ac80d806-6f8e-4a60-a9f7-c9ea6f81aa91"
      },
      "source": [
        "# Step 1 - Loading the data from our pipeline outputs for exploration\n",
        "The code below will install the necessary dependencies in order to get started exploring the full dataset post data ingestion pipeline. You will need to run this first before the data is loaded into the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6824c05f-f77a-4d95-86d7-f67e649c6061",
      "metadata": {
        "id": "6824c05f-f77a-4d95-86d7-f67e649c6061"
      },
      "outputs": [],
      "source": [
        "!pip install gdown==4.6.0\n",
        "!pip install pyarrow==10.0.1\n",
        "!pip install mlflow==2.1.1\n",
        "!pip install pyngrok 5.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1741ef6-2ef7-4491-b55f-11f0e1e242e0",
      "metadata": {
        "id": "d1741ef6-2ef7-4491-b55f-11f0e1e242e0"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Location of publicly available combined set for analysis post data ingestion pipeline run\n",
        "url = \"https://drive.google.com/drive/folders/1tUcqzoeM9AaDaUexcHg-OqXJeL6f5k6U?usp=share_link\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb9d6cf-b8cb-4b33-a89f-56d386f02653",
      "metadata": {
        "id": "7eb9d6cf-b8cb-4b33-a89f-56d386f02653"
      },
      "outputs": [],
      "source": [
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "\n",
        "combined_data = pq.read_table(source=\"who_ears_social_listening_public/merged_owid_who_ears.parquet\").to_pandas()\n",
        "# We will need to reformat the date column for analysis\n",
        "combined_data['date'] = pd.to_datetime(combined_data['date'], format = '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df67e88b-47ef-4530-abd2-47f7227e42a3",
      "metadata": {
        "id": "df67e88b-47ef-4530-abd2-47f7227e42a3"
      },
      "outputs": [],
      "source": [
        "combined_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ed1259-6161-486e-be16-ed8cd49d33ce",
      "metadata": {
        "id": "f5ed1259-6161-486e-be16-ed8cd49d33ce"
      },
      "outputs": [],
      "source": [
        "# This will show the first 10 columns and rows we just want to make sure we loaded the data. \n",
        "combined_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8feq2ZoWGHLL"
      },
      "source": [
        "# Step 2 - Creating a Space for Data Exploration \n",
        "We want to give the ability for our Domain expert to explore all of the variables in order to determine the most interesting to answer the research question. Leveraging tools like dtale make it easy to examine things such as missing variable and correlations. \n"
      ],
      "id": "8feq2ZoWGHLL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dtale package\n",
        "# Note: You may have to restart the runtime in which case be sure and import the dataframe again from the above code. \n",
        "!pip install -U dtale\n",
        "!pip install statsmodels --upgrade"
      ],
      "metadata": {
        "id": "VtdepE9DGwoa"
      },
      "id": "VtdepE9DGwoa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine out dataset \n",
        "import pandas as pd\n",
        "\n",
        "import dtale\n",
        "import dtale.app as dtale_app\n",
        "\n",
        "dtale_app.USE_COLAB = True # Comment this out if you are using another environment\n",
        "\n",
        "dtale.show(combined_data)"
      ],
      "metadata": {
        "id": "0QOaalNyG-Fg"
      },
      "id": "0QOaalNyG-Fg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the link above and lets analyze our data and select the right features! You might also want to get acquainted with the dtale documentation here https://github.com/man-group/dtale. "
      ],
      "metadata": {
        "id": "J2pmM7oLJK4q"
      },
      "id": "J2pmM7oLJK4q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHzFRzfCKAY4"
      },
      "source": [
        "# Step 3 - Feature Engineering - Extraction and Transformation\n",
        "Now that we know which columns we want to extract we can pull these out. We have some clever insights from our domain expert who has instucted us to perform the following transformations on the variables in preparation for the time series model analysis. Once we experiment with this and we feel comfortable we are on the right track we can build this in the pipeline in prepartion for production. \n",
        "\n",
        "Tranform Step 1 - Create a dataframe with the following columns: \n",
        "\n",
        "* date\n",
        "* mis_and_disinformation\n",
        "* mis_and_disinformation_male\n",
        "* mis_and_disinformation_female\n",
        "* myths\n",
        "* myths_female\n",
        "* myths_male\n",
        "* new_vaccinations_smoothed\n",
        "\n",
        "Tranform Step 2 - Merge all myths and mis_disinformation for all columns respectively by summing the columns. The new columns names will will be prefixed with 'mis_myths_' (e.g. 'mis_myths_male', 'mis_myths_female' etc.) "
      ],
      "id": "xHzFRzfCKAY4"
    },
    {
      "cell_type": "code",
      "source": [
        "# lets next create a sensible feature set for training and testing \n",
        "features = combined_data[['date', 'mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "                          'mis_and_disinformation_female',\n",
        "                         'myths','myths_female', 'myths_male', 'new_vaccinations_smoothed']]\n",
        "\n",
        "# remove any NaN\n",
        "features = features.dropna()\n",
        "\n",
        "# Sum all the variables and rename\n",
        "features = features.eval(\"mis_myths = myths + mis_and_disinformation\")\n",
        "features = features.eval(\"mis_myths_male = myths_male + mis_and_disinformation_male\")\n",
        "features = features.eval(\"mis_myths_female = myths_female + mis_and_disinformation_female\")\n",
        "\n",
        "# Lets drop the old columns now that we have merged the two into our new columns\n",
        "\n",
        "features = features.drop(columns=['mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "                          'mis_and_disinformation_female',\n",
        "                         'myths','myths_female', 'myths_male'])\n",
        "\n",
        "features.head(10)"
      ],
      "metadata": {
        "id": "eZ6yT8DENe4p"
      },
      "id": "eZ6yT8DENe4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1o4nAAAPeNo"
      },
      "source": [
        "# Step 4 - Selecting the optimal model for prediction \n",
        "Since this is time series data and we can reasonaly infer starting with analysis of performance on linear models. As we don't want to spend time writing boilerplate code lets start with a library that simply uses generic hyperparameters for a multitude of regression models and displays the results for us. \n"
      ],
      "id": "Z1o4nAAAPeNo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sadly there are a lot of dependencies that conflict with dtale so we need to remove them and install pycaret\n",
        "!pip uninstall -y dtale\n",
        "!pip uninstall -y statsmodels\n",
        "# install pycaret as a way to do some quick analysis of all regression model types to know which performs the best \n",
        "!pip install --pre pycaret\n"
      ],
      "metadata": {
        "id": "6NsesepZQMRe"
      },
      "id": "6NsesepZQMRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.regression import *\n",
        "# lets setup our first session for eval\n",
        "# we chose total cases since its a complete set AND is has high correlation with the other features (Pearson)\n",
        "# our X is all the data from the who-ears social media listening set and our Y \n",
        "# or target is the new_vaccinations_smoothed to predict\n",
        "\n",
        "session_1 = setup(features, target = 'new_vaccinations_smoothed', \n",
        "                  session_id=1, \n",
        "                  log_experiment=False, \n",
        "                  experiment_name='new_vaccinations_smoothed_1')"
      ],
      "metadata": {
        "id": "oHdcryBCTIPa"
      },
      "id": "oHdcryBCTIPa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now compare all the regression models available \n",
        "best_model = compare_models()"
      ],
      "metadata": {
        "id": "rRP-cWXYTSWa"
      },
      "id": "rRP-cWXYTSWa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IXnqrCdgpo4"
      },
      "source": [
        "# Step 5 - Post Initial Model Analysis and Experimentation\n",
        "We weren't able to get a very high R2 with our features, but after some discussion we think we may be able to improve the initial numbers before entering the hyperparametering tuning phase. We have now been instructed to consider some additional OWID columns: hosp_patients, new_cases_smoothed and new_deaths_smoothed and then analyze these new columns by shifting on 0, 5, 10 and 20 days in the four variables. This is done due to the lag in COVID-19 data since someone can see the misinformation, make a decision to not vaccinate then days later become infected. Again this is just an experiment so we don't know if it will yield better outcomes.  \n"
      ],
      "id": "9IXnqrCdgpo4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we know that we are going to have to keep repeating this experiment in \n",
        "# many forms with many variables lets be smart about it and build some methods \n",
        "# to make our life easier. \n",
        "\n",
        "# Lets define our target and our target shift in days \n",
        "target_name = 'new_cases_smoothed'\n",
        "shift_in_days = 0\n",
        "\n",
        "# First lets create a new feature set with our desired training and target base\n",
        "# features \n",
        "def create_feature_set(df, columns):\n",
        "    # Create a feature set by selecting specified columns from the dataframe\n",
        "    features = df[columns]\n",
        "    \n",
        "    # Remove any NaN values\n",
        "    features = features.dropna()\n",
        "    \n",
        "    # Sum all the variables and rename\n",
        "    features = features.eval(\"mis_myths = myths + mis_and_disinformation\")\n",
        "    features = features.eval(\"mis_myths_male = myths_male + mis_and_disinformation_male\")\n",
        "    features = features.eval(\"mis_myths_female = myths_female + mis_and_disinformation_female\")\n",
        "    # Drop the old columns now that we have merged the two into our new columns\n",
        "    features = features.drop(columns=['mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "                          'mis_and_disinformation_female',\n",
        "                         'myths','myths_female', 'myths_male'])\n",
        "    return features\n",
        "\n",
        "# now we can run these accordingly \n",
        "columns = ['date', 'mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "                          'mis_and_disinformation_female',\n",
        "                         'myths','myths_female', 'myths_male', target_name]\n",
        "feature_set = create_feature_set(combined_data, columns)\n",
        "\n",
        "# Lets construct a method that takes a list of columns from the master combinded\n",
        "# dataset, a target column name (such as new_vaccinations_smoothed) and a shift\n",
        "# value such as 5, 10 or 20 days. Since we cannot use NaN columns lets drop those\n",
        "# rows created from a shift\n",
        "\n",
        "def shift_merge_and_dropna(df, target, shift_value):\n",
        "    # Shift the target column by the shift value\n",
        "    df_shifted = pd.Series(df[target].shift(shift_value).values, index=df[\"date\"], copy=False)\n",
        "    \n",
        "    # Merge the original dataframe with the shifted dataframe on the index\n",
        "    df_merged = pd.merge(df, df_shifted.rename(target+'_'+str(shift_value)), how='right', on=df.index)\n",
        "    \n",
        "    # Drop the NaN created from the shift \n",
        "    df_merged = df_merged.dropna()\n",
        "\n",
        "    # Drop the original field and the key index which we used to join so that we \n",
        "    # dont use that as part of our training set. \n",
        "    df_merged = df_merged.drop(columns=[target, 'key_0'])\n",
        "    return df_merged\n",
        "\n",
        "# lets give it a little test\n",
        "df_output = shift_merge_and_dropna(feature_set, target_name, shift_in_days)\n",
        "\n",
        "df_output.head(20)\n"
      ],
      "metadata": {
        "id": "Fntumq7d6afS"
      },
      "id": "Fntumq7d6afS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.regression import *\n",
        "# lets setup our first session for eval as we did before\n",
        "\n",
        "session_1 = setup(df_output, target = target_name+'_'+str(shift_in_days), \n",
        "                  session_id=shift_in_days, \n",
        "                  log_experiment=False, \n",
        "                  experiment_name=target_name+'_'+str(shift_in_days))\n",
        "\n",
        "# We can now compare all the regression models available \n",
        "best_model = compare_models()"
      ],
      "metadata": {
        "id": "MApFM_KC5DAk"
      },
      "id": "MApFM_KC5DAk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 - Experimentation Tracking and Analysis\n",
        "Those methods are really helpful and we are able to run one experiment and one shift at a time, but thats pretty daunting to do manually. If only there were a way to run all the experiments and then put them in a table so we can understand what is the best approach forward. Lets meet our new best friend MLflow!"
      ],
      "metadata": {
        "id": "yb_ot7BGIVNE"
      },
      "id": "yb_ot7BGIVNE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup NGROK so we can share this with others and view the MLflow UI \n",
        "import mlflow\n",
        "\n",
        "# Push tracking UI to run in the background\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\") \n",
        "\n",
        "# Setup a remote tunnel using ngrok.com to allow local port access\n",
        "# borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6\n",
        "\n",
        "# import the required libraries \n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"{Insert your auth token here once you signup from the link above}\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "RKI7RhEVCkVN"
      },
      "id": "RKI7RhEVCkVN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['new_cases_smoothed', 'hosp_patients', 'new_vaccinations_smoothed', 'new_deaths_smoothed']\n",
        "shift_in_days = [0,5,10,20]\n",
        "\n",
        "# Define columns once before the loop\n",
        "columns = ['date', 'mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "           'mis_and_disinformation_female', 'myths','myths_female', 'myths_male']\n",
        "\n",
        "# Loop through the target names and shift values\n",
        "for target_name in target_names:\n",
        "  # Append target name to columns list\n",
        "  columns.append(target_name)\n",
        "  for shift in shift_in_days:\n",
        "\n",
        "    # Create feature set using the updated columns list\n",
        "    feature_set = create_feature_set(combined_data, columns)\n",
        "\n",
        "    # do the shift and merge to create the new feature\n",
        "    feature_set = shift_merge_and_dropna(feature_set, target_name, shift)\n",
        "\n",
        "    # Setup session with target name appended with shift value and experiment name appended with shift value\n",
        "    session_1 = setup(feature_set, target = target_name + '_' + str(shift), \n",
        "                      session_id=shift, \n",
        "                      log_experiment=True, \n",
        "                      experiment_name=target_name + '_' + str(shift))\n",
        "\n",
        "    # Compare all the regression models\n",
        "    best_model = compare_models()\n",
        "    \n",
        "  # Remove target name from columns list for next iteration\n",
        "  columns.remove(target_name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wAssY5xZM-l-"
      },
      "id": "wAssY5xZM-l-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}