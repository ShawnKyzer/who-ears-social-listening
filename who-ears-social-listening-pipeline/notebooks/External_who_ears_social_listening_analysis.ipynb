{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ac80d806-6f8e-4a60-a9f7-c9ea6f81aa91",
      "metadata": {
        "id": "ac80d806-6f8e-4a60-a9f7-c9ea6f81aa91"
      },
      "source": [
        "# Step 1 - Loading the data from our pipeline outputs for exploration\n",
        "The code below will install the necessary dependencies in order to get started exploring the full dataset post data ingestion pipeline. You will need to run this first before the data is loaded into the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6824c05f-f77a-4d95-86d7-f67e649c6061",
      "metadata": {
        "id": "6824c05f-f77a-4d95-86d7-f67e649c6061"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1741ef6-2ef7-4491-b55f-11f0e1e242e0",
      "metadata": {
        "id": "d1741ef6-2ef7-4491-b55f-11f0e1e242e0"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Location of publicly available combined set for analysis post data ingestion pipeline run\n",
        "url = \"https://drive.google.com/drive/folders/1tUcqzoeM9AaDaUexcHg-OqXJeL6f5k6U?usp=share_link\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb9d6cf-b8cb-4b33-a89f-56d386f02653",
      "metadata": {
        "id": "7eb9d6cf-b8cb-4b33-a89f-56d386f02653"
      },
      "outputs": [],
      "source": [
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "\n",
        "combined_data = pq.read_table(source=\"who_ears_social_listening_public/merged_owid_who_ears.parquet\").to_pandas()\n",
        "# We will need to reformat the date column for analysis\n",
        "combined_data['date'] = pd.to_datetime(combined_data['date'], format = '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df67e88b-47ef-4530-abd2-47f7227e42a3",
      "metadata": {
        "id": "df67e88b-47ef-4530-abd2-47f7227e42a3"
      },
      "outputs": [],
      "source": [
        "combined_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ed1259-6161-486e-be16-ed8cd49d33ce",
      "metadata": {
        "id": "f5ed1259-6161-486e-be16-ed8cd49d33ce"
      },
      "outputs": [],
      "source": [
        "# This will show the first 10 columns and rows we just want to make sure we loaded the data. \n",
        "combined_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8feq2ZoWGHLL"
      },
      "source": [
        "# Step 2 - Creating a Space for Data Exploration \n",
        "We want to give the ability for our Domain expert to explore all of the variables in order to determine the most interesting to answer the research question. Leveraging tools like dtale make it easy to examine things such as missing variable and correlations. \n"
      ],
      "id": "8feq2ZoWGHLL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dtale package\n",
        "# Note: You may have to restart the runtime in which case be sure and import the dataframe again from the above code. \n",
        "!pip install -U dtale\n",
        "!pip install statsmodels --upgrade"
      ],
      "metadata": {
        "id": "VtdepE9DGwoa"
      },
      "id": "VtdepE9DGwoa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine out dataset \n",
        "import pandas as pd\n",
        "\n",
        "import dtale\n",
        "import dtale.app as dtale_app\n",
        "\n",
        "dtale_app.USE_COLAB = True # Comment this out if you are using another environment\n",
        "\n",
        "dtale.show(combined_data)"
      ],
      "metadata": {
        "id": "0QOaalNyG-Fg"
      },
      "id": "0QOaalNyG-Fg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the link above and lets analyze our data and select the right features! You might also want to get acquainted with the dtale documentation here https://github.com/man-group/dtale. "
      ],
      "metadata": {
        "id": "J2pmM7oLJK4q"
      },
      "id": "J2pmM7oLJK4q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHzFRzfCKAY4"
      },
      "source": [
        "# Step 3 - Feature Engineering - Extraction and Transformation\n",
        "Now that we know which columns we want to extract we can pull these out. We have some clever insights from our domain expert who has instucted us to perform the following transformations on the variables in preparation for the time series model analysis. Once we experiment with this and we feel comfortable we are on the right track we can build this in the pipeline in prepartion for production. \n",
        "\n",
        "Tranform Step 1 - Create a dataframe with the following columns: \n",
        "\n",
        "* date\n",
        "* mis_and_disinformation\n",
        "* mis_and_disinformation_male\n",
        "* mis_and_disinformation_female\n",
        "* myths\n",
        "* myths_female\n",
        "* myths_male\n",
        "* new_vaccinations_smoothed\n",
        "\n",
        "Tranform Step 2 - Merge all myths and mis_disinformation for all columns respectively by summing the columns. The new columns names will will be prefixed with 'mis_myths_' (e.g. 'mis_myths_male', 'mis_myths_female' etc.) "
      ],
      "id": "xHzFRzfCKAY4"
    },
    {
      "cell_type": "code",
      "source": [
        "# lets next create a sensible feature set for training and testing \n",
        "features = combined_data[['date', 'mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "                          'mis_and_disinformation_female',\n",
        "                         'myths','myths_female', 'myths_male', 'new_vaccinations_smoothed']]\n",
        "\n",
        "# remove any NaN\n",
        "features = features.dropna()\n",
        "\n",
        "# Sum all the variables and rename\n",
        "features = features.eval(\"mis_myths = myths + mis_and_disinformation\")\n",
        "features = features.eval(\"mis_myths_male = myths_male + mis_and_disinformation_male\")\n",
        "features = features.eval(\"mis_myths_female = myths_female + mis_and_disinformation_female\")\n",
        "\n",
        "# Lets drop the old columns now that we have merged the two into our new columns\n",
        "\n",
        "features = features.drop(columns=['mis_and_disinformation', 'mis_and_disinformation_male',  \n",
        "                          'mis_and_disinformation_female',\n",
        "                         'myths','myths_female', 'myths_male'])\n",
        "\n",
        "features.head(10)"
      ],
      "metadata": {
        "id": "eZ6yT8DENe4p"
      },
      "id": "eZ6yT8DENe4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1o4nAAAPeNo"
      },
      "source": [
        "# Step 4 - Selecting the optimal model for prediction \n",
        "Since this is time series data and we can reasonaly infer starting with analysis of performance on linear models. As we don't want to spend time writing boilerplate code lets start with a library that simply uses generic hyperparameters for a multitude of regression models and displays the results for us. \n"
      ],
      "id": "Z1o4nAAAPeNo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sadly there are a lot of dependencies that conflict with dtale so we need to remove them and install pycaret\n",
        "!pip uninstall -y dtale\n",
        "!pip uninstall -y statsmodels\n",
        "# install pycaret as a way to do some quick analysis of all regression model types to know which performs the best \n",
        "!pip install --pre pycaret\n"
      ],
      "metadata": {
        "id": "6NsesepZQMRe"
      },
      "id": "6NsesepZQMRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.regression import *\n",
        "# lets setup our first session for eval\n",
        "# we chose total cases since its a complete set AND is has high correlation with the other features (Pearson)\n",
        "# our X is all the data from the who-ears social media listening set and our Y \n",
        "# or target is the new_vaccinations_smoothed to predict\n",
        "\n",
        "session_1 = setup(features, target = 'new_vaccinations_smoothed', \n",
        "                  session_id=1, \n",
        "                  log_experiment=False, \n",
        "                  experiment_name='new_vaccinations_smoothed_1')"
      ],
      "metadata": {
        "id": "oHdcryBCTIPa"
      },
      "id": "oHdcryBCTIPa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now compare all the regression models available \n",
        "best_model = compare_models()"
      ],
      "metadata": {
        "id": "rRP-cWXYTSWa"
      },
      "id": "rRP-cWXYTSWa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}